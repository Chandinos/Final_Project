[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Introduction\nThe goal of this analysis is to fit and tune classifiers that predict diabetes status using the survey data. The dataset originates from a large public health survey program, the Behavioral Risk Factor Surveillance System, collected and published by the Centers for Disease Control and Prevention. Since predictors are mixed-type and may relate to diabetes in non-linear ways, we compare two popular tree-based classifiers: a single classification tree model and a random forest, both fit using modeling workflows from the tidymodels package ecosystem. Models are tuned using log-loss so predictions that confidently assign the correct class are rewarded and over-confident wrong predictions are penalized.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\n\nset.seed(987)\n\n#Read in cleaned data from EDA\ndiabetes &lt;- read_rds(\"data/diabetes_clean.rds\")\n\n\n\nData Split\nAfter loading, we split the data into 70% training and 30% testing to evaluate generalization performance. A seed ensures reproducibility across models.\n\n#Split Data\ndiab_split &lt;- initial_split(diabetes, prop = 0.7, strata = diabetes_binary)\ntrain_data &lt;- training(diab_split)\ntest_data &lt;- testing(diab_split)\n\n\n\nClassification Tree\nA classification tree splits data into regions using predictor values to separate diabetes groups. We tune the cost complexity parameter to balance tree size and generalization, since our EDA showed skew and imbalance that could lead to overfitting without pruning.\n\n#Specifying classification tree model\ntree_spec &lt;- decision_tree(\n  cost_complexity = tune(),\n  min_n = 20\n) |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"rpart\")\n\n#Workflow (6 predictors)\ntree_wf &lt;- workflow() |&gt;\n  add_model(tree_spec) |&gt;\n  add_formula(\n    diabetes_binary ~ bmi + age + high_bp + \n      high_chol + phys_activity + smoker\n  )\n\n#5-fold cross-validation (on training set)\ntree_folds &lt;- vfold_cv(train_data, v=5, strata = diabetes_binary)\n\n#Tuning grid (for cost complexity)\ntree_grid &lt;- grid_regular(\n  cost_complexity(),\n  levels = 10\n)\n\n#Tuning the tree using log-loss\ntree_tune &lt;- tune_grid(\n  tree_wf,\n  resamples = tree_folds,\n  grid = tree_grid,\n  metrics = metric_set(mn_log_loss),\n  control = control_grid(save_pred = TRUE)\n)\n\n#Looking at performance \ntree_metrics &lt;- collect_metrics(tree_tune)\ntree_metrics\n\n# A tibble: 10 × 7\n   cost_complexity .metric     .estimator  mean     n    std_err .config        \n             &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;          \n 1    0.0000000001 mn_log_loss binary     0.349     5 0.00107    pre0_mod01_pos…\n 2    0.000000001  mn_log_loss binary     0.349     5 0.00107    pre0_mod02_pos…\n 3    0.00000001   mn_log_loss binary     0.349     5 0.00107    pre0_mod03_pos…\n 4    0.0000001    mn_log_loss binary     0.349     5 0.00107    pre0_mod04_pos…\n 5    0.000001     mn_log_loss binary     0.349     5 0.00107    pre0_mod05_pos…\n 6    0.00001      mn_log_loss binary     0.351     5 0.000965   pre0_mod06_pos…\n 7    0.0001       mn_log_loss binary     0.358     5 0.000711   pre0_mod07_pos…\n 8    0.001        mn_log_loss binary     0.358     5 0.000777   pre0_mod08_pos…\n 9    0.01         mn_log_loss binary     0.404     5 0.00000973 pre0_mod09_pos…\n10    0.1          mn_log_loss binary     0.404     5 0.00000973 pre0_mod10_pos…\n\n#Selecting lowest log-loss and finalizing work flow\nbest_tree &lt;- select_best(tree_tune, metric = \"mn_log_loss\")\nbest_tree\n\n# A tibble: 1 × 2\n  cost_complexity .config         \n            &lt;dbl&gt; &lt;chr&gt;           \n1    0.0000000001 pre0_mod01_post0\n\nfinal_tree_wf &lt;- finalize_workflow(tree_wf, best_tree)\n\n#Fit final tree on training data \ntree_final_fit &lt;- last_fit(final_tree_wf, diab_split)\n\n#Test-set performance\ntree_final_fit |&gt; collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 accuracy    binary         0.860 pre0_mod0_post0\n2 roc_auc     binary         0.752 pre0_mod0_post0\n3 brier_class binary         0.106 pre0_mod0_post0\n\n#Compute log-loss on test set for the final tree\ntree_logloss &lt;- tree_final_fit |&gt;\n  collect_predictions() |&gt;\n  mn_log_loss(truth = diabetes_binary,\n              .pred_Yes,\n              event_level = \"second\")\n\ntree_logloss\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary         0.353\n\n\nTest log-loss matches training (≈ 0.35), so the pruned tree generalized well without obvious overfit. We keep this as our best single-tree model and move to random forest tuning.\n\n\nRandom Forest\nA random forest extends the idea of a single classification tree by fitting many trees on bootstrap samples of the training data and averaging their predictions. At each split, a random subset of predictors is considered, which decorrelates the trees and reduces variance relative to a single tree. This makes random forests less interpretable but typically more accurate and stable. Here, we tune the mtry parameter, which controls how many predictors are available at each split, using 5-fold cross-validation and log-loss on the training data.\n\n#Specifying Random Forest Model\nrf_spec &lt;- rand_forest(\n  mtry  = tune(),\n  trees = 100,\n  min_n = 20\n) |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"ranger\", importance = \"impurity\")\n\n#Workflow (same 6 predictors as Classification Tree)\nrf_wf &lt;- workflow() |&gt;\n  add_model(rf_spec) |&gt;\n  add_formula(\n    diabetes_binary ~ bmi + age + high_bp +\n      high_chol + phys_activity + smoker\n  )\n\n#5-fold cross-validation (on training set)\nrf_folds &lt;- vfold_cv(train_data, v = 5, strata = diabetes_binary)\n\n#Tuning grid for mtry\nrf_grid &lt;- tibble(mtry = 2:6)\n\n#Tuning random forest using log-loss\nrf_tune &lt;- tune_grid(\n  rf_wf,\n  resamples = rf_folds,\n  grid = rf_grid,\n  metrics = metric_set(mn_log_loss),\n  control = control_grid(save_pred = TRUE)\n)\n\n#Look at performance (mtry values)\nrf_metrics &lt;- collect_metrics(rf_tune)\nrf_metrics\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n  std_err .config        \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;          \n1     2 mn_log_loss binary     0.336     5 0.000912 pre0_mod1_post0\n2     3 mn_log_loss binary     0.336     5 0.00111  pre0_mod2_post0\n3     4 mn_log_loss binary     0.342     5 0.00121  pre0_mod3_post0\n4     5 mn_log_loss binary     0.351     5 0.00196  pre0_mod4_post0\n5     6 mn_log_loss binary     0.408     5 0.00478  pre0_mod5_post0\n\n#Selecting lowest log-loss (best mtry) and finalize workflow\nrf_best &lt;- select_best(rf_tune, metric = \"mn_log_loss\")\nrf_best\n\n# A tibble: 1 × 2\n   mtry .config        \n  &lt;int&gt; &lt;chr&gt;          \n1     2 pre0_mod1_post0\n\nrf_final_wf &lt;- finalize_workflow(rf_wf, rf_best)\n\n#Fit final random forest on training/test split\nrf_final_fit &lt;- last_fit(rf_final_wf, diab_split)\n\n#Test-set performance\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 accuracy    binary         0.862 pre0_mod0_post0\n2 roc_auc     binary         0.790 pre0_mod0_post0\n3 brier_class binary         0.103 pre0_mod0_post0\n\n#Compute log-loss on test set for the final random forest\nrf_logloss &lt;- rf_final_fit |&gt;\n  collect_predictions() |&gt;\n  mn_log_loss(truth = diabetes_binary,\n              .pred_Yes,\n              event_level = \"second\")\n\nrf_logloss\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary         0.337\n\n\n\n\nFinal Model Selection\nWe compare our best pruned classification tree and tuned random forest, selecting the strongest test-set performer as our final mode.\n\n#Collect the test-set metrics into objects\ntree_test_metrics &lt;- tree_final_fit |&gt; collect_metrics()\nrf_test_metrics   &lt;- rf_final_fit   |&gt; collect_metrics()\n\n#Extract test-set log-loss values too\ntree_log &lt;- tree_final_fit |&gt; collect_predictions() |&gt; mn_log_loss(diabetes_binary, .pred_Yes, event_level = \"second\")\nrf_log   &lt;- rf_final_fit   |&gt; collect_predictions() |&gt; mn_log_loss(diabetes_binary, .pred_Yes, event_level = \"second\")\n\n#Putting metrics into a single table\ncompare &lt;- bind_rows(\n  tree_test_metrics |&gt; mutate(model = \"Classification Tree\"),\n  rf_test_metrics   |&gt; mutate(model = \"Random Forest\")\n) |&gt;\n  select(model, .metric, .estimate)\n\n#Adding log-loss (not part of default `collect_metrics()` output)\ncompare &lt;- bind_rows(\n  compare,\n  tibble(model = \"Classification Tree\", .metric = \"mn_log_loss\", .estimate = tree_log$.estimate),\n  tibble(model = \"Random Forest\",       .metric = \"mn_log_loss\", .estimate = rf_log$.estimate)\n)\n\ncompare\n\n# A tibble: 8 × 3\n  model               .metric     .estimate\n  &lt;chr&gt;               &lt;chr&gt;           &lt;dbl&gt;\n1 Classification Tree accuracy        0.860\n2 Classification Tree roc_auc         0.752\n3 Classification Tree brier_class     0.106\n4 Random Forest       accuracy        0.862\n5 Random Forest       roc_auc         0.790\n6 Random Forest       brier_class     0.103\n7 Classification Tree mn_log_loss     0.353\n8 Random Forest       mn_log_loss     0.337\n\ncompare |&gt; knitr::kable(digits = 3)\n\n\n\n\nmodel\n.metric\n.estimate\n\n\n\n\nClassification Tree\naccuracy\n0.860\n\n\nClassification Tree\nroc_auc\n0.752\n\n\nClassification Tree\nbrier_class\n0.106\n\n\nRandom Forest\naccuracy\n0.862\n\n\nRandom Forest\nroc_auc\n0.790\n\n\nRandom Forest\nbrier_class\n0.103\n\n\nClassification Tree\nmn_log_loss\n0.353\n\n\nRandom Forest\nmn_log_loss\n0.337\n\n\n\n\n\nThe Random Forest model is selected as the overall winner due to superior ROC AUC and lower log-loss with comparable accuracy."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Introduction\nThe purpose of this exploratory data analysis (EDA) is to develop an understanding of the Diabetes Health Indicators BRFSS 2015 dataset. This dataset contains a variety of demographic, behavioral, and clinical health variables collected through the Behavioral Risk Factor Surveillance System.\nAfter completing the EDA, the next step will be to build models that predict whether a person has diabetes using the variables in this dataset. Our goal is to compare different modeling approaches to see which one predicts the binary diabetes indicator (0 = No, 1 = Yes) most accurately. Because the predictors include a mix of demographic, behavioral, and clinical information, we will test models that can handle different types of data and look for patterns that might help improve prediction performance.\n\n\nData Overview\n\n#Basic structure and first few rows of the dataset\nhead(diabetes)\n\n# A tibble: 6 × 22\n  diabetes_binary high_bp high_chol chol_check   bmi smoker stroke\n            &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0       1         1          1    40      1      0\n2               0       0         0          0    25      1      0\n3               0       1         1          1    28      0      0\n4               0       1         0          1    27      0      0\n5               0       1         1          1    24      0      0\n6               0       1         1          1    25      1      0\n# ℹ 15 more variables: heart_diseaseor_attack &lt;dbl&gt;, phys_activity &lt;dbl&gt;,\n#   fruits &lt;dbl&gt;, veggies &lt;dbl&gt;, hvy_alcohol_consump &lt;dbl&gt;,\n#   any_healthcare &lt;dbl&gt;, no_docbc_cost &lt;dbl&gt;, gen_hlth &lt;dbl&gt;, ment_hlth &lt;dbl&gt;,\n#   phys_hlth &lt;dbl&gt;, diff_walk &lt;dbl&gt;, sex &lt;dbl&gt;, age &lt;dbl&gt;, education &lt;dbl&gt;,\n#   income &lt;dbl&gt;\n\nglimpse(diabetes)\n\nRows: 253,680\nColumns: 22\n$ diabetes_binary        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,…\n$ high_bp                &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,…\n$ high_chol              &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,…\n$ chol_check             &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ bmi                    &lt;dbl&gt; 40, 25, 28, 27, 24, 25, 30, 25, 30, 24, 25, 34,…\n$ smoker                 &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,…\n$ stroke                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ heart_diseaseor_attack &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ phys_activity          &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,…\n$ fruits                 &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,…\n$ veggies                &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ hvy_alcohol_consump    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ any_healthcare         &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ no_docbc_cost          &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ gen_hlth               &lt;dbl&gt; 5, 3, 5, 2, 2, 2, 3, 3, 5, 2, 3, 3, 3, 4, 4, 2,…\n$ ment_hlth              &lt;dbl&gt; 18, 0, 30, 0, 3, 0, 0, 0, 30, 0, 0, 0, 0, 0, 30…\n$ phys_hlth              &lt;dbl&gt; 15, 0, 30, 0, 0, 2, 14, 0, 30, 0, 0, 30, 15, 0,…\n$ diff_walk              &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,…\n$ sex                    &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,…\n$ age                    &lt;dbl&gt; 9, 7, 9, 11, 11, 10, 9, 11, 9, 8, 13, 10, 7, 11…\n$ education              &lt;dbl&gt; 4, 6, 4, 3, 5, 6, 6, 4, 5, 4, 6, 5, 5, 4, 6, 6,…\n$ income                 &lt;dbl&gt; 3, 1, 8, 6, 4, 8, 7, 4, 1, 3, 8, 1, 7, 6, 2, 8,…\n\n\nThe Diabetes Health Indicators BRFSS 2015 dataset contains a large collection of health-related variables gathered from adult survey respondents. Each row represents an individual, and each column corresponds to a specific health, demographic, or behavioral indicator.\nThis dataset includes: + diabetes_binary variable whether an individual has diabetes or not (our goal is to predict this variable) + Behavioral factors such as smoking status and physical activity. + Demographic variables such as sex and age category. + Clinical indicators such as BMI, high cholesterol, and high blood pressure.\nThe dataset includes different types of variables—numbers, yes/no indicators, and ordered categories—so it’s important to understand what each one looks like before modeling. The following sections explore these variables, how they are distributed, and how they connect to diabetes.\n\n\nVariable Type Conversion and Missingness Check\nBecause the dataset contains a mixture of binary health indicators, ordinal survey responses, and continuous measurements, we first convert key variables into appropriate factor types with interpretable labels. This step ensures that later summaries and models treat these variables correctly (for example, not treating 0/1 indicators as continuous). We also quantify missing values to confirm that the dataset is sufficiently complete for modeling.\n\n#Convert 0/1 indicators and coded categories to factors with labels\ndiabetes &lt;- diabetes |&gt; \n  mutate( \n    diabetes_binary = factor(diabetes_binary,\n                             levels = c(0,1),\n                             labels = c(\"No\", \"Yes\")),\n    #Binary indicators\n    high_bp = factor(high_bp, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    high_chol = factor(high_chol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    chol_check = factor(chol_check, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    smoker = factor(smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    stroke = factor(stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    heart_diseaseor_attack = factor(heart_diseaseor_attack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    phys_activity = factor(phys_activity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    fruits = factor(fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    veggies = factor(veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    hvy_alcohol_consump = factor(hvy_alcohol_consump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    any_healthcare = factor(any_healthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    no_docbc_cost = factor(no_docbc_cost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    diff_walk = factor(diff_walk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    sex = factor(sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n    \n    #Ordinal health status (self-reported)\n    gen_hlth = factor(\n      gen_hlth,\n      levels = 1:5,\n      labels = c(\"Excellent\", \"Very good\", \"Good\", \"Fair\", \"Poor\"), \n      ordered = TRUE\n    ),\n    \n    #Ordinal age (BRFSS coding)\n    age = factor(\n      age,\n      levels = 1:13,\n      labels = c(\n        \"18–24\", \"25–29\", \"30–34\", \"35–39\",\n        \"40–44\", \"45–49\", \"50–54\", \"55–59\",\n        \"60–64\", \"65–69\", \"70–74\", \"75–79\", \"80+\"),\n      ordered = TRUE\n    ),\n    \n    #Ordinal education\n    education = factor(\n      education,\n      levels = 1:6, \n      labels = c( \"No school/kindergarten\", \"Grades 1–8\", \"Grades 9–11\", \n                  \"High school/GED\", \"Some college\", \"College grad\" ), \n      ordered = TRUE \n      ),\n    \n    #Ordinal income\n    income = factor( \n      income, \n      levels = 1:8, \n      labels = c( \"&lt;$10k\", \"$10–15k\", \"$15–20k\", \"$20–25k\", \n                  \"$25–35k\", \"$35–50k\", \"$50–75k\", \"&gt;$75k\" ), \n      ordered = TRUE \n      )\n  )\n\n#Summarize missingness\nmissing_summary &lt;- diabetes |&gt;\n  summarize(across(everything(), ~sum(is.na(.)))) |&gt;\n  pivot_longer(everything(), \n               names_to = \"variable\",\n               values_to = \"n_missing\") |&gt;\n  mutate(pct_missing = n_missing / nrow(diabetes)) |&gt;\n  arrange(desc(n_missing))\n\nmissing_summary\n\n# A tibble: 22 × 3\n   variable               n_missing pct_missing\n   &lt;chr&gt;                      &lt;int&gt;       &lt;dbl&gt;\n 1 diabetes_binary                0           0\n 2 high_bp                        0           0\n 3 high_chol                      0           0\n 4 chol_check                     0           0\n 5 bmi                            0           0\n 6 smoker                         0           0\n 7 stroke                         0           0\n 8 heart_diseaseor_attack         0           0\n 9 phys_activity                  0           0\n10 fruits                         0           0\n# ℹ 12 more rows\n\n#Saving cleaned data set\nwrite_rds(diabetes, \"data/diabetes_clean.rds\")\n\nThe missingness summary indicates that most variables have very few (or no) missing values, so we proceed without additional imputation at the EDA stage.\n\n\nDistributional Summaries\nTo prepare for visual inspection and modeling, we summarize continuous and categorical variables separately, focusing on central tendency, spread, and level frequencies.\n\n#Categorical summaries for key factors\ncat_summary &lt;- diabetes |&gt;\n  select(diabetes_binary, high_bp, high_chol, smoker,\n          phys_activity, sex, age, gen_hlth, education, income) |&gt;\n  mutate(across(everything(), ~as.factor(as.character(.)))) |&gt;\n  pivot_longer(cols = everything(),\n               names_to = \"variable\",\n               values_to = \"level\") |&gt;\n  count(variable, level, name = \"n\") |&gt;\n  group_by(variable) |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n  ungroup()\n\ncat_summary\n\n# A tibble: 44 × 4\n   variable level     n    pct\n   &lt;chr&gt;    &lt;fct&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 age      18–24  5700 0.0225\n 2 age      25–29  7598 0.0300\n 3 age      30–34 11123 0.0438\n 4 age      35–39 13823 0.0545\n 5 age      40–44 16157 0.0637\n 6 age      45–49 19819 0.0781\n 7 age      50–54 26314 0.104 \n 8 age      55–59 30832 0.122 \n 9 age      60–64 33244 0.131 \n10 age      65–69 32194 0.127 \n# ℹ 34 more rows\n\n#Numeric Summaries\nnum_summary &lt;- diabetes |&gt;\n  select(where(is.numeric)) |&gt;\n  pivot_longer(cols = everything(),\n               names_to = \"variable\",\n               values_to = \"value\") |&gt;\n  group_by(variable) |&gt;\n  summarise(\n    mean = mean(value, na.rm = TRUE),\n    sd = sd(value, na.rm = TRUE),\n    min = min(value, na.rm = TRUE),\n    q25 = quantile(value, 0.25, na.rm = TRUE),\n    med = median(value, na.rm = TRUE),\n    q75 = quantile(value, 0.75, na.rm = TRUE),\n    max = max(value, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nnum_summary\n\n# A tibble: 3 × 8\n  variable   mean    sd   min   q25   med   q75   max\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 bmi       28.4   6.61    12    24    27    31    98\n2 ment_hlth  3.18  7.41     0     0     0     2    30\n3 phys_hlth  4.24  8.72     0     0     0     3    30\n\n\nContinuous predictors such as BMI, mental health days, and physical health days show right-skewed distributions, with medians equal to 0 for both health measures, indicating many respondents report no unhealthy days despite higher means. BMI ranges from 12 to 98, with a median of 27 and mean near 28.4, suggesting moderate skew and some extreme values. Age group frequencies confirm smaller representation in young adult ranges (18–39), and heavier concentration in middle and older adult ranges (50–74), which later supports exploring non-linear modeling methods that handle skew and mixed data types without transformation.\n\n\nCategorical Variable Balance\nMany of the predictors in this dataset are binary or ordered categorical variables, such as blood pressure status, general health, age group, and income. Before modeling, it is useful to look at how responses are distributed across levels to see which categories are common, which are rare, and whether there is any serious imbalance that might affect how models learn from these predictors.\n\n#Faceted bar plots for key variables \ndiabetes |&gt;\n  select(diabetes_binary, high_bp, high_chol, smoker,\n         phys_activity, sex) |&gt;\n  mutate(across(everything(), as.character)) |&gt;\n  pivot_longer(cols = everything(),\n               names_to = \"variable\",\n               values_to = \"level\") |&gt;\n  ggplot(aes(x = level)) +\n  geom_bar() +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(x = NULL, y = \"Count\",\n       title = \"Categorical variable level counts\")\n\n\n\n\n\n\n\n#Individual Bar Plots for remaining variables (to ensure clean visuals)\n#Age\ndiabetes |&gt; \n  ggplot(aes(x = age)) +\n  geom_bar() +\n  labs(x = \"Age Category\", y = \"Count\",\n       title = \"Age Level Counts\")\n\n\n\n\n\n\n\n#Education\ndiabetes |&gt;\n  ggplot(aes(x = education)) +\n  geom_bar() +\n  labs(x = \"Education Level\", y = \"Count\",\n       title = \"Education Level Counts\")\n\n\n\n\n\n\n\n#General Health\ndiabetes |&gt;\nggplot(aes(x = gen_hlth)) +\n  geom_bar() +\n  labs(x = \"General Health\", y = \"Count\",\n       title = \"General Health Level Counts\")\n\n\n\n\n\n\n\n#Income Level \ndiabetes |&gt;\n  ggplot(aes(x = income)) +\n  geom_bar() +\n  labs(x = \"Income Bracket\", y = \"Count\",\n       title = \"Income Level Counts\")\n\n\n\n\n\n\n\n\nIn short, the categorical predictors are clean and usable. The binary flags are balanced, while the ordinal categories are skewed toward older and higher-income respondents with some sparse levels. This supports evaluating flexible, non-linear models later.\n\n\nBivariate Relationships with Diabetes Status\nTo understand how individual predictors differ across diabetes status, we compare key categorical variables using cross-tabulated proportions, and examine numeric measures by outcome group to check for differences in central tendency and distributional overlap.\n\ndiabetes_by_binary &lt;- diabetes |&gt;\n  select(diabetes_binary, high_bp, high_chol, smoker, phys_activity) |&gt;\n  pivot_longer(cols = -diabetes_binary,\n               names_to = \"predictor\",\n               values_to = \"response\") |&gt;\n  group_by(predictor, diabetes_binary, response) |&gt;\n  summarise(n = n(), .groups = \"drop_last\") |&gt;\n  group_by(predictor, diabetes_binary) |&gt;\n  mutate(pct = round(n / sum(n), 4)) |&gt;\n  ungroup()\n\ndiabetes_by_binary\n\n# A tibble: 16 × 5\n   predictor     diabetes_binary response      n   pct\n   &lt;chr&gt;         &lt;fct&gt;           &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n 1 high_bp       No              No       136109 0.623\n 2 high_bp       No              Yes       82225 0.377\n 3 high_bp       Yes             No         8742 0.247\n 4 high_bp       Yes             Yes       26604 0.753\n 5 high_chol     No              No       134429 0.616\n 6 high_chol     No              Yes       83905 0.384\n 7 high_chol     Yes             No        11660 0.330\n 8 high_chol     Yes             Yes       23686 0.670\n 9 phys_activity No              No        48701 0.223\n10 phys_activity No              Yes      169633 0.777\n11 phys_activity Yes             No        13059 0.370\n12 phys_activity Yes             Yes       22287 0.630\n13 smoker        No              No       124228 0.569\n14 smoker        No              Yes       94106 0.431\n15 smoker        Yes             No        17029 0.482\n16 smoker        Yes             Yes       18317 0.518\n\n\nThe bivariate analysis shows strong separation for clinical flags, especially high blood pressure (75% in diabetics vs. 38% in non-diabetics) and high cholesterol (67% vs. 38%). Physical activity trends suggest lower activity among diabetics (63% vs. 78%), indicating potential predictive value for inactivity. Smoking differences are smaller (52% vs. 43%) but both levels are well represented, so smoking may still help models that capture interactions.\n\n\nCorrelation Analysis for Numeric Measures\nFor numeric predictors, we briefly inspect correlations with diabetes status and among predictors to detect strong linear signals and possible multicollinearity.\n\n#Encoding diabetes_binary as 0/1 to test for correlation\ndiabetes_encode &lt;- diabetes |&gt; mutate(diab = as.numeric(diabetes_binary == \"Yes\"))\n\n#Correlation with diabetes and numeric predictors\ndiabetes_corr &lt;- diabetes_encode |&gt;\n  select(diab, bmi, ment_hlth, phys_hlth) |&gt;\n  cor(use = \"pairwise.complete.obs\") |&gt;\n  as_tibble(rownames = \"variable\")\n\ndiabetes_corr\n\n# A tibble: 4 × 5\n  variable    diab    bmi ment_hlth phys_hlth\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 diab      1      0.217     0.0693     0.171\n2 bmi       0.217  1         0.0853     0.121\n3 ment_hlth 0.0693 0.0853    1          0.354\n4 phys_hlth 0.171  0.121     0.354      1    \n\n\nBMI has the strongest linear association with diabetes (r = 0.217), suggesting higher BMI is more common among diabetics. Physical health days show a small signal (r = 0.171), while mental health alone is weak (r = 0.693). Correlation between health variables is moderate (r = 0.354) but not redundant, so we retain all features for modeling.\n\n\nKey EDA Insights for Modeling\nEDA shows that diabetes has moderate numeric association with BMI and strong separation across clinical binaries like high BP and cholesterol. Many ordinal variables (age, education, income, and general health) are skewed toward higher levels but remain usable and complete enough to retain for modeling. Smoking and physical activity show smaller but well-represented differences, so they may still add value in interaction-aware or tree-based learners. Overall, the BRFSS survey data is clean and modeling-ready, motivating comparisons between interpretable linear models from Tidymodels workflows and flexible tree or forest approaches built using engines like rpart and ranger.\n\n\nModeling Page Link\nClick here for the Modeling page"
  }
]